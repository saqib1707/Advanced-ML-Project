{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch_normalization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NPEcGabHdL5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmG2LgY1lYMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# print('Show system RAM memory\\n\\n')\n",
        "# !cat /proc/meminfo | egrep 'MemTotal*'\n",
        "# print('\\nShow devices : \\n'+str(device_lib.list_local_devices()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DUAsJcm0cZTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# print(\"TF version : \", tf.__version__)\n",
        "# print(\"TF-GPU available? \", tf.test.is_gpu_available())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RX9MPK1ieShz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 50\n",
        "\n",
        "dataset_id = 1          # 1 - MNIST, 2 - Cifar10\n",
        "if dataset_id == 1:\n",
        "  img_size = [28,28,1]\n",
        "else:\n",
        "  img_size = [32,32,3]\n",
        "num_fc1_units = 1000\n",
        "num_classes = 10\n",
        "num_filter_layer1 = 32\n",
        "num_filter_layer2 = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phMKR2q6qUzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_mnist():\n",
        "  from tensorflow.examples.tutorials.mnist import input_data\n",
        "  mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "  train_x = np.reshape(mnist.train.images, (mnist.train.labels.shape[0], img_size[0], img_size[1], img_size[2]))\n",
        "  train_y = mnist.train.labels\n",
        "  test_x = np.reshape(mnist.test.images, (mnist.test.labels.shape[0], img_size[0], img_size[1], img_size[2]))\n",
        "  test_y = mnist.test.labels\n",
        "  return (train_x, train_y, test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CX-GcZTSIWbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_cifar10():\n",
        "  (train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "  # preprocessing the input image features\n",
        "  temp1 = np.reshape(train_x,(train_x.shape[0],img_size[0]*img_size[1]*img_size[2]))\n",
        "  temp2 = np.reshape(test_x,(test_x.shape[0],img_size[0]*img_size[1]*img_size[2]))\n",
        "  mean = np.mean(temp1,axis=0)\n",
        "  stddev = np.std(temp1,axis=0)\n",
        "  temp1 = (temp1 - mean)/stddev\n",
        "  temp2 = (temp2 - mean)/stddev\n",
        "  train_x = np.reshape(temp1,(train_x.shape[0],img_size[0],img_size[1],img_size[2]))\n",
        "  test_x = np.reshape(temp2,(test_x.shape[0],img_size[0],img_size[1],img_size[2]))\n",
        "  \n",
        "  # converting labels to one-hot encoding\n",
        "  temp1 = np.zeros((train_y.shape[0],10),dtype=np.float32)\n",
        "  temp2 = np.zeros((test_y.shape[0],10),dtype=np.float32)\n",
        "  temp1[np.arange(train_y.shape[0]),np.reshape(train_y,(train_y.shape[0]))] = 1.0\n",
        "  temp2[np.arange(test_y.shape[0]),np.reshape(test_y,(test_y.shape[0]))] = 1.0\n",
        "  return (train_x, temp1, test_x, temp2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFgGzeEpe1rv",
        "colab_type": "code",
        "outputId": "06445da9-d399-47b9-87ca-238ffbea33ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, train_y, test_x, test_y = data_mnist() if dataset_id == 1 else data.cifar10()\n",
        "x = tf.placeholder(tf.float32, [None, img_size[0], img_size[1], img_size[2]], name=\"data\")\n",
        "y = tf.placeholder(tf.float32, [None, num_classes], name=\"labels\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwitRsVfWalm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape, mu=0, sigma=1.0):\n",
        "\tweight = tf.truncated_normal(shape=shape, mean=mu, stddev=sigma, dtype=tf.float32)\n",
        "\treturn tf.Variable(weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BryiapsYcEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bias_variable(shape, mu=0, sigma=1.0):\n",
        "  bias = tf.truncated_normal(shape=shape, mean=mu, stddev=sigma, dtype=tf.float32)\n",
        "  return tf.Variable(bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cqUVbB0iLQPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_normalization_fc(input_tensor):\n",
        "  variance_epsilon = 1e-10\n",
        "  input_tensor = tf.cast(input_tensor, dtype=tf.float32)\n",
        "  moments = tf.nn.moments(input_tensor, axes=0)     # moment return mean and variance of the tensor\n",
        "  norm_tensor = tf.divide(tf.subtract(input_tensor, moments[0]), tf.sqrt(tf.add(moments[1], variance_epsilon)))\n",
        "  gamma = weight_variable(shape=tf.shape(moments[0]))\n",
        "  beta = weight_variable(shape=tf.shape(moments[0]))\n",
        "  out_tensor = tf.add(tf.multiply(norm_tensor, gamma), beta)\n",
        "  return out_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdqFOheK8WJO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_normalization_conv(input_tensor):\n",
        "  variance_epsilon = 1e-10\n",
        "  input_tensor = tf.cast(input_tensor, dtype=tf.float32)\n",
        "  moments = tf.nn.moments(input_tensor, axes=0)\n",
        "  norm_tensor = tf.divide(tf.subtract(input_tensor, moments[0]), tf.sqrt(tf.add(moments[1], variance_epsilon)))\n",
        "  gamma = weight_variable(shape=tf.shape(moments[0]))\n",
        "  beta = weight_variable(shape=tf.shape(moments[0]))\n",
        "#   out_tensor = tf.nn.batch_normalization(x=input_tensor,mean=moments[0],variance=moments[1],offset=beta,scale=gamma,variance_epsilon=1e-10)\n",
        "  out_tensor = tf.add(tf.multiply(norm_tensor, gamma), beta)\n",
        "  return out_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWfEUBSBxO4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def group_norm(input_tensor, G=32, eps=1e-5, scope='group_norm') :\n",
        "    with tf.variable_scope(scope) :\n",
        "        N, H, W, C = input_tensor.get_shape().as_list()\n",
        "        G = min(G, C)\n",
        "\n",
        "        input_tensor = tf.reshape(input_tensor, [N, H, W, G, C // G])\n",
        "        mean, var = tf.nn.moments(input_tensor, [1, 2, 4], keep_dims=True)\n",
        "        input_tensor = (input_tensor - mean) / tf.sqrt(var + eps)\n",
        "\n",
        "        gamma = tf.get_variable('gamma', [1, 1, 1, C], initializer=tf.constant_initializer(1.0))\n",
        "        beta = tf.get_variable('beta', [1, 1, 1, C], initializer=tf.constant_initializer(0.0))\n",
        "        input_tensor = tf.reshape(input_tensor, [N, H, W, C]) * gamma + beta\n",
        "\n",
        "    return input_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvmxeFxi4U1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below function is taken from https://github.com/tensorpack/tensorpack/blob/79148350eabd6800133a49b101eea4c56e78e4e8/examples/ImageNetModels/vgg16.py#L19-L46"
      ]
    },
    {
      "metadata": {
        "id": "xjpWQhXJ4Pul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GroupNorm(x, group, gamma_initializer=tf.constant_initializer(1.)):\n",
        "    \"\"\"\n",
        "    https://arxiv.org/abs/1803.08494\n",
        "    \"\"\"\n",
        "    x = tf.transpose(x, [0,3,1,2])\n",
        "    shape = x.get_shape().as_list()\n",
        "    ndims = len(shape)\n",
        "    assert ndims == 4, shape\n",
        "    chan = shape[1]\n",
        "    assert chan % group == 0, chan\n",
        "    group_size = chan // group\n",
        "\n",
        "    orig_shape = tf.shape(x)\n",
        "    h, w = orig_shape[2], orig_shape[3]\n",
        "\n",
        "    x = tf.reshape(x, tf.stack([-1, group, group_size, h, w]))\n",
        "\n",
        "    mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n",
        "\n",
        "    new_shape = [1, group, group_size, 1, 1]\n",
        "\n",
        "#     beta1 = tf.get_variable('beta1', [chan], initializer=tf.constant_initializer())\n",
        "#     beta1 = tf.reshape(beta1, new_shape)\n",
        "\n",
        "#     gamma1 = tf.get_variable('gamma1', [chan], initializer=gamma_initializer)\n",
        "#     gamma1 = tf.reshape(gamma1, new_shape)\n",
        "    gamma = weight_variable(shape=new_shape)\n",
        "    beta = weight_variable(shape=new_shape)\n",
        "\n",
        "    out = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-5, name='output')\n",
        "    out = tf.reshape(out, orig_shape, name='output')\n",
        "    return tf.transpose(out, [0,2,3,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzKsHwwae49h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
        "    # setup the filter input shape for tf.nn.conv_2d\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
        "\n",
        "    # initialise weights and bias for the filter\n",
        "    weights = tf.Variable(tf.truncated_normal(shape=conv_filt_shape, stddev=0.03), name=name+'_W')\n",
        "    bias = tf.Variable(tf.truncated_normal(shape=[num_filters]), name=name+'_b')\n",
        "\n",
        "    # setup the convolutional layer operation\n",
        "    out_layer = tf.nn.conv2d(input=input_data, filter=weights, strides=[1, 1, 1, 1], padding='SAME') + bias\n",
        "\n",
        "#     out_layer = batch_normalization_conv(out_layer)\n",
        "#     out_layer = tf.contrib.layers.batch_norm(out_layer)\n",
        "    out_layer = GroupNorm(out_layer,group=16)\n",
        "\n",
        "    # apply a ReLU non-linear activation\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "\n",
        "    # now perform max pooling\n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "    pool_strides = [1, 2, 2, 1]\n",
        "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=pool_strides, padding='SAME')\n",
        "\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViIcSy6Yd3N7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_next_batch(step, batch_size):\n",
        "    return train_x[step*batch_size:(step+1)*batch_size,:,:,:], train_y[step*batch_size:(step+1)*batch_size,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpwgbr0Sy1Rm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_fc_layer(input_tensor, num_input_units, num_output_units):\n",
        "  weight = weight_variable(shape=[num_input_units, num_output_units])\n",
        "  bias = bias_variable(shape=[num_output_units])\n",
        "  output_tensor = tf.add(tf.matmul(input_tensor, weight), bias)\n",
        "#   output_tensor = batch_normalization_fc(output_tensor)\n",
        "  output_tensor = tf.contrib.layers.batch_norm(output_tensor)\n",
        "  return output_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9tr3Lmse7hK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create some convolutional layers\n",
        "layer1 = create_conv_layer(x, img_size[2], num_filter_layer1, [5, 5], [2, 2], name='layer1')\n",
        "mod_img_size = [img_size[0]//2, img_size[1]//2, img_size[2]]\n",
        "layer2 = create_conv_layer(layer1, num_filter_layer1, num_filter_layer2, [5, 5], [2, 2], name='layer2')\n",
        "mod_img_size = [mod_img_size[0]//2, mod_img_size[1]//2, mod_img_size[2]]\n",
        "\n",
        "flattened = tf.reshape(layer2, [-1, mod_img_size[0]*mod_img_size[1]*num_filter_layer2])\n",
        "\n",
        "# setup some weights and bias values for this layer, then activate with ReLU\n",
        "dense_layer_1 = tf.nn.relu(create_fc_layer(input_tensor=flattened, num_input_units=mod_img_size[0]*mod_img_size[1]*num_filter_layer2, num_output_units=num_fc1_units))\n",
        "# another layer with softmax activations\n",
        "dense_layer_2 = create_fc_layer(input_tensor=dense_layer_1, num_input_units=num_fc1_units, num_output_units=num_classes)\n",
        "y_predicted = tf.nn.softmax(dense_layer_2)\n",
        "\n",
        "with tf.name_scope(\"loss_value\"):\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer_2, labels=y))\n",
        "tf.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "# add an optimizer\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "# define an accuracy assessment operation\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_predicted, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "# setup recording variables\n",
        "merged_summary = tf.summary.merge_all()\n",
        "# test_writer = tf.summary.FileWriter('/tmp/mnist_demo/test/1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80wU_wpxfASi",
        "colab_type": "code",
        "outputId": "e7ff8568-f160-49be-cd90-4f5b646ca50a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "iteration_step = 1\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "#     train_writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
        "    total_batch = int(train_y.shape[0]/batch_size)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for itr in range(total_batch):\n",
        "#             if iteration_step % 10 == 0:\n",
        "#                 test_accuracy, loss_value = sess.run([accuracy, cross_entropy], feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "#                 test_writer.add_summary(summary, iteration_step)\n",
        "#                 print('test accuracy at step %s: %s' % (iteration_step, test_accuracy))\n",
        "\n",
        "            batch_x, batch_y = get_next_batch(step=itr, batch_size=batch_size)\n",
        "            _ = sess.run([optimizer], feed_dict={x:batch_x, y:batch_y})\n",
        "#             train_writer.add_summary(summary, iteration_step)\n",
        "            iteration_step += 1\n",
        "#             print(iteration_step)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(\"final test accuracy\", sess.run(accuracy, feed_dict={x: test_x, y: test_y}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete!\n",
            "final test accuracy 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}